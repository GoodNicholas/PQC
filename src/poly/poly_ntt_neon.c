/**
 * @file poly_ntt_saber_complete.c
 * @brief Complete Saber NTT multiplication - Based on TCHES 2021 paper
 *
 * Reference: "NTT Multiplication for NTT-unfriendly Rings: New Speed Records 
 *            for Saber and NTRU on Cortex-M4 and AVX2"
 *            Chi-Ming Marvin Chung et al., TCHES 2021
 *
 * KEY INSIGHT FROM PAPER:
 * Saber uses q = 8192 (power of 2, NOT NTT-friendly).
 * 
 * SOLUTION (Section 3.1 of paper):
 * 1. Lift coefficients: Z_8192 → Z_q' (where q' is NTT-friendly prime)
 * 2. Perform **incomplete** NTT:
 *    - 6 layers of radix-2 NTT (256 → 4 coefficients per block)
 *    - Leaves 64 blocks of 4 coefficients each
 * 3. Multiply 4×4 blocks using optimized schoolbook
 * 4. Inverse incomplete NTT (6 layers)
 * 5. Reduce: Z_q' → Z_8192
 *
 * PRIMES USED (from paper, Table on page 10):
 * - Lightsaber: q' = 20972417 = 163847 × 128 + 1
 * - Saber/Firesaber: q' = 25166081 = 196610 × 128 + 1
 *
 * WHY 6 LAYERS? (Section 3.1, page 10-11):
 * "We achieve the best performance when doing 6 layers of NTTs"
 * - Comparison shows 4×4 schoolbook beats size-4 NTT on Cortex-M4
 * - Balances NTT overhead vs schoolbook cost
 *
 * PERFORMANCE (from paper, Table 5):
 * - MatrixVectorMul: 61% faster than Toom-Cook
 * - Overall Saber encapsulation: 22% faster
 */

 #include "../../include/poly.h"
 #include "../../include/params.h"
 #include <string.h>
 #include <stdint.h>
 
 #ifdef __ARM_NEON
 #include <arm_neon.h>
 #define USE_NEON 1
 #else
 #define USE_NEON 0
 #endif
 
 /* ========================================================================
  * NTT Parameters
  * ======================================================================== */
 
 // Use Saber parameters (also works for Firesaber)
 // For Lightsaber, would need different constants
 
 #define Q_PRIME 25166081
 #define Q_PRIME_INV 41877759U  // -q'^{-1} mod 2^32
 #define NTT_LAYERS 6
 #define OMEGA 14966334
 
 // Number of blocks after incomplete NTT
 // After 6 layers: 256 / 2^6 = 4 coefficients per block
 // Total blocks: 256 / 4 = 64
 #define BLOCK_SIZE 4
 #define NUM_BLOCKS 64
 
 /* ========================================================================
  * Twiddle Factors (Generated by generate_saber_ntt_constants.py)
  * ======================================================================== */
 
 // Forward NTT twiddle factors (Montgomery form)
 static const int32_t zetas_ntt[32] = {
     13858533, 5563113, 4092287, 21905048, 18234579, 22655248, 11980428, 9567042,
     24577585, 16472287, 7460755, 22443020, 24346825, 17101524, 15565412, 10575964,
     15932507, 12174351, 1069349, 5184115, 19847105, 1134236, 24551809, 18929621,
     16472356, 10171507, 10319196, 6726360, 15663744, 10910265, 14660437, 25123393,
 };
 
 // Inverse NTT twiddle factors (Montgomery form)
 static const int32_t zetas_inv_ntt[32] = {
     42688, 10505644, 14255816, 9502337, 18439721, 14846885, 14994574, 8693725,
     6236460, 614272, 24031845, 5318976, 19981966, 24096732, 12991730, 9233574,
     14590117, 9600669, 8064557, 819256, 2723061, 17705326, 8693794, 588496,
     15599039, 13185653, 2510833, 6931502, 3261033, 21073794, 19602968, 11307548,
 };
 
 // Normalization: (2^NTT_LAYERS)^{-1} mod q' in Montgomery form
 // For 6 layers: 64^{-1} mod q'
 static const int32_t NINV_MONT = 16776702;
 
 /* ========================================================================
  * Montgomery Arithmetic
  * 
  * Montgomery reduction: compute a*R^{-1} mod q' where R = 2^32
  * From paper Algorithm 10 (page 9)
  * ======================================================================== */
 
 static inline int32_t montgomery_reduce(int64_t a)
 {
     // Algorithm: a*R^{-1} mod q' = (a - (a*q'^{-1} mod R)*q') / R
     int32_t t = (int32_t)((uint64_t)a * Q_PRIME_INV);
     t = (a - (int64_t)t * Q_PRIME) >> 32;
     return t;  // Result in [-q', q']
 }
 
 static inline int32_t barrett_reduce(int32_t a)
 {
     // Reduce to [0, q')
     while (a >= Q_PRIME) a -= Q_PRIME;
     while (a < 0) a += Q_PRIME;
     return a;
 }
 
 static inline int32_t fqmul(int32_t a, int32_t b)
 {
     // Multiply a*b in Montgomery domain
     return montgomery_reduce((int64_t)a * b);
 }
 
 /* ========================================================================
  * NEON Montgomery Arithmetic (if available)
  * ======================================================================== */
 
 #if USE_NEON
 
 static inline int32x4_t montgomery_reduce_neon(int64x2_t lo, int64x2_t hi)
 {
     // Extract low 32 bits
     int32x2_t t_lo = vmovn_s64(lo);
     int32x2_t t_hi = vmovn_s64(hi);
     
     // m = a * Q_PRIME_INV
     int32x2_t m_lo = vmul_n_s32(t_lo, (int32_t)Q_PRIME_INV);
     int32x2_t m_hi = vmul_n_s32(t_hi, (int32_t)Q_PRIME_INV);
     
     // Compute (a - m*q') >> 32
     int32x2_t q_vec = vdup_n_s32(Q_PRIME);
     int64x2_t prod_lo = vmull_s32(m_lo, q_vec);
     int64x2_t prod_hi = vmull_s32(m_hi, q_vec);
     
     lo = vsubq_s64(lo, prod_lo);
     hi = vsubq_s64(hi, prod_hi);
     
     int32x2_t res_lo = vshrn_n_s64(lo, 32);
     int32x2_t res_hi = vshrn_n_s64(hi, 32);
     
     return vcombine_s32(res_lo, res_hi);
 }
 
 static inline int32x4_t fqmul_neon(int32x4_t a, int32x4_t b)
 {
     int64x2_t prod_lo = vmull_s32(vget_low_s32(a), vget_low_s32(b));
     int64x2_t prod_hi = vmull_s32(vget_high_s32(a), vget_high_s32(b));
     return montgomery_reduce_neon(prod_lo, prod_hi);
 }
 
 static inline int32x4_t barrett_reduce_neon(int32x4_t a)
 {
     int32x4_t q_vec = vdupq_n_s32(Q_PRIME);
     
     // Two-pass reduction
     uint32x4_t mask_hi = vcgeq_s32(a, q_vec);
     uint32x4_t mask_lo = vcltq_s32(a, vdupq_n_s32(0));
     
     int32x4_t sub = vandq_s32(vreinterpretq_s32_u32(mask_hi), q_vec);
     int32x4_t add = vandq_s32(vreinterpretq_s32_u32(mask_lo), q_vec);
     
     a = vsubq_s32(a, sub);
     a = vaddq_s32(a, add);
     
     // Second pass
     mask_hi = vcgeq_s32(a, q_vec);
     sub = vandq_s32(vreinterpretq_s32_u32(mask_hi), q_vec);
     a = vsubq_s32(a, sub);
     
     return a;
 }
 
 #endif // USE_NEON
 
 /* ========================================================================
  * Lift/Reduce between Z_8192 and Z_q'
  * ======================================================================== */
 
 static void poly_lift(int32_t out[SABER_N], const uint16_t in[SABER_N])
 {
     // Lift from Z_8192 to Z_q'
     for (int i = 0; i < SABER_N; i++) {
         out[i] = (int32_t)in[i];
     }
 }
 
 static void poly_reduce_saber(uint16_t out[SABER_N], const int32_t in[SABER_N])
 {
     // Reduce Z_q' → Z_8192
     for (int i = 0; i < SABER_N; i++) {
         int32_t val = barrett_reduce(in[i]);
         out[i] = (uint16_t)(val & (SABER_Q - 1));  // mod 8192
     }
 }
 
 /* ========================================================================
  * Incomplete NTT - Forward Transform
  * 
  * Performs 6 layers of Cooley-Tukey butterflies
  * Input: 256 coefficients
  * Output: 64 blocks of 4 coefficients each (in NTT domain)
  * 
  * Based on paper Algorithm 19 (page 29) - merged layers approach
  * ======================================================================== */
 
 /**
  * ntt_incomplete_forward - 6-layer incomplete NTT
  * 
  * Layer structure (Cooley-Tukey):
  * Layer 0: len=128, 1 butterfly group
  * Layer 1: len=64,  2 butterfly groups
  * Layer 2: len=32,  4 butterfly groups
  * Layer 3: len=16,  8 butterfly groups
  * Layer 4: len=8,   16 butterfly groups
  * Layer 5: len=4,   32 butterfly groups
  * 
  * We merge layers 0-2 (first 3), then layers 3-5 (next 3)
  * as suggested in the paper for optimal register usage.
  */
 static void ntt_incomplete_forward(int32_t a[SABER_N])
 {
     unsigned int len, start, j, k = 0;
     
     // 6 layers total, process in groups for better performance
     for (len = 128; len >= 4; len >>= 1) {
         for (start = 0; start < SABER_N; start = j + len) {
             int32_t zeta = zetas_ntt[k++];
             
             for (j = start; j < start + len; ++j) {
                 // Cooley-Tukey butterfly:
                 // t = zeta * a[j + len]
                 // a[j + len] = a[j] - t
                 // a[j] = a[j] + t
                 int32_t t = fqmul(zeta, a[j + len]);
                 a[j + len] = a[j] - t;
                 a[j] = a[j] + t;
             }
         }
     }
     
     // After 6 layers: we have 64 blocks of 4 coefficients
     // Each block is in NTT domain modulo (x^4 - zeta_i)
 }
 
 /* ========================================================================
  * Incomplete NTT - Inverse Transform
  * 
  * Performs 6 layers of Gentleman-Sande butterflies (in reverse order)
  * Input: 64 blocks of 4 coefficients (in NTT domain)
  * Output: 256 coefficients
  * 
  * Uses Gentleman-Sande (GS) butterflies for better final stage
  * (from paper, Section 3.1)
  * ======================================================================== */
 
 static void ntt_incomplete_inverse(int32_t a[SABER_N])
 {
     unsigned int len, start, j, k = 0;
     
     // Inverse: 6 layers in reverse order
     for (len = 4; len <= 128; len <<= 1) {
         for (start = 0; start < SABER_N; start = j + len) {
             int32_t zeta = zetas_inv_ntt[k++];
             
             for (j = start; j < start + len; ++j) {
                 // Gentleman-Sande butterfly:
                 // t = a[j]
                 // a[j] = t + a[j + len]
                 // a[j + len] = zeta * (t - a[j + len])
                 int32_t t = a[j];
                 a[j] = t + a[j + len];
                 a[j + len] = fqmul(zeta, t - a[j + len]);
             }
         }
     }
     
     // Multiply by N^{-1} = 64^{-1}
     for (j = 0; j < SABER_N; ++j) {
         a[j] = fqmul(a[j], NINV_MONT);
         a[j] = barrett_reduce(a[j]);
     }
 }
 
 /* ========================================================================
  * NEON-Optimized NTT (if ARM NEON available)
  * ======================================================================== */
 
 #if USE_NEON
 
 /**
  * ntt_incomplete_forward_neon - NEON-optimized 6-layer NTT
  * 
  * Vectorizes layers where len >= 4 (process 4 butterflies at once)
  */
 static void ntt_incomplete_forward_neon(int32_t a[SABER_N])
 {
     unsigned int len, start, j, k = 0;
     
     // Large layers: vectorize (len >= 16)
     for (len = 128; len >= 16; len >>= 1) {
         for (start = 0; start < SABER_N; start += 2 * len) {
             int32_t zeta_scalar = zetas_ntt[k++];
             int32x4_t zeta_vec = vdupq_n_s32(zeta_scalar);
             
             for (j = start; j < start + len; j += 4) {
                 int32x4_t a_vec = vld1q_s32(&a[j]);
                 int32x4_t b_vec = vld1q_s32(&a[j + len]);
                 
                 int32x4_t t = fqmul_neon(zeta_vec, b_vec);
                 
                 vst1q_s32(&a[j + len], vsubq_s32(a_vec, t));
                 vst1q_s32(&a[j], vaddq_s32(a_vec, t));
             }
         }
     }
     
     // Small layers: scalar (len < 16)
     for (len = 8; len >= 4; len >>= 1) {
         for (start = 0; start < SABER_N; start = j + len) {
             int32_t zeta = zetas_ntt[k++];
             for (j = start; j < start + len; ++j) {
                 int32_t t = fqmul(zeta, a[j + len]);
                 a[j + len] = a[j] - t;
                 a[j] = a[j] + t;
             }
         }
     }
 }
 
 static void ntt_incomplete_inverse_neon(int32_t a[SABER_N])
 {
     unsigned int len, start, j, k = 0;
     
     // Small layers: scalar
     for (len = 4; len < 16; len <<= 1) {
         for (start = 0; start < SABER_N; start = j + len) {
             int32_t zeta = zetas_inv_ntt[k++];
             for (j = start; j < start + len; ++j) {
                 int32_t t = a[j];
                 a[j] = t + a[j + len];
                 a[j + len] = fqmul(zeta, t - a[j + len]);
             }
         }
     }
     
     // Large layers: vectorize
     for (len = 16; len <= 128; len <<= 1) {
         for (start = 0; start < SABER_N; start += 2 * len) {
             int32_t zeta_scalar = zetas_inv_ntt[k++];
             int32x4_t zeta_vec = vdupq_n_s32(zeta_scalar);
             
             for (j = start; j < start + len; j += 4) {
                 int32x4_t a_vec = vld1q_s32(&a[j]);
                 int32x4_t b_vec = vld1q_s32(&a[j + len]);
                 
                 int32x4_t t = a_vec;
                 vst1q_s32(&a[j], vaddq_s32(a_vec, b_vec));
                 
                 int32x4_t diff = vsubq_s32(t, b_vec);
                 vst1q_s32(&a[j + len], fqmul_neon(zeta_vec, diff));
             }
         }
     }
     
     // Normalization with NEON
     int32x4_t ninv_vec = vdupq_n_s32(NINV_MONT);
     for (j = 0; j < SABER_N; j += 4) {
         int32x4_t a_vec = vld1q_s32(&a[j]);
         a_vec = fqmul_neon(a_vec, ninv_vec);
         a_vec = barrett_reduce_neon(a_vec);
         vst1q_s32(&a[j], a_vec);
     }
 }
 
 #endif // USE_NEON
 /* ========================================================================
  * 4×4 Schoolbook Multiplication (Optimized)
  * 
  * This is the KEY optimization from the paper (Section 3.1, Algorithm 18)!
  * 
  * INSIGHT: For matrix-vector product, we compute many inner products.
  * Instead of: multiply → reduce → accumulate
  * We do: accumulate in 64-bit → reduce once
  * 
  * This saves montgomeryR operations and adds!
  * ======================================================================== */
 
 /**
  * schoolbook_4x4_accumulate - Optimized 4×4 block multiplication
  * 
  * Computes: r += a * b (mod x^4 - zeta)
  * 
  * From paper Algorithm 18 (page 28):
  * "We can absorb all adds into the multiplication, changing some smull 
  *  into smlal. This saves adds and montgomeryR operations."
  *
  * @param r[4] - accumulator (will be updated)
  * @param a[4] - first polynomial
  * @param b[4] - second polynomial  
  * @param zeta - root of unity for this block
  */
 static void schoolbook_4x4_accumulate(int32_t r[4], 
                                       const int32_t a[4],
                                       const int32_t b[4],
                                       int32_t zeta)
 {
     // For polynomial multiplication mod (x^4 - zeta):
     // r[0] = a[0]*b[0] + zeta*(a[1]*b[3] + a[2]*b[2] + a[3]*b[1])
     // r[1] = a[0]*b[1] + a[1]*b[0] + zeta*(a[2]*b[3] + a[3]*b[2])
     // r[2] = a[0]*b[2] + a[1]*b[1] + a[2]*b[0] + zeta*a[3]*b[3]
     // r[3] = a[0]*b[3] + a[1]*b[2] + a[2]*b[1] + a[3]*b[0]
     
     // Paper's optimization: accumulate in 64-bit, reduce once
     
     // r[0]
     int64_t acc = (int64_t)a[1] * b[3];
     acc += (int64_t)a[2] * b[2];
     acc += (int64_t)a[3] * b[1];
     int32_t tmp = montgomery_reduce(acc);
     acc = (int64_t)tmp * zeta;
     acc += (int64_t)a[0] * b[0];
     r[0] += montgomery_reduce(acc);
     
     // r[1]
     acc = (int64_t)a[2] * b[3];
     acc += (int64_t)a[3] * b[2];
     tmp = montgomery_reduce(acc);
     acc = (int64_t)tmp * zeta;
     acc += (int64_t)a[0] * b[1];
     acc += (int64_t)a[1] * b[0];
     r[1] += montgomery_reduce(acc);
     
     // r[2]
     acc = (int64_t)a[3] * b[3];
     tmp = montgomery_reduce(acc);
     acc = (int64_t)tmp * zeta;
     acc += (int64_t)a[0] * b[2];
     acc += (int64_t)a[1] * b[1];
     acc += (int64_t)a[2] * b[0];
     r[2] += montgomery_reduce(acc);
     
     // r[3]
     acc = (int64_t)a[0] * b[3];
     acc += (int64_t)a[1] * b[2];
     acc += (int64_t)a[2] * b[1];
     acc += (int64_t)a[3] * b[0];
     r[3] += montgomery_reduce(acc);
 }
 
 /**
  * pointwise_mul_ntt - Multiply polynomials in NTT domain
  * 
  * After incomplete NTT, we have 64 blocks of 4 coefficients.
  * Each block represents polynomial mod (x^4 - zeta_i).
  * We multiply corresponding blocks using schoolbook.
  */
 static void pointwise_mul_ntt(int32_t r[SABER_N],
                               const int32_t a[SABER_N],
                               const int32_t b[SABER_N])
 {
     // Compute appropriate zetas for each block
     // For incomplete NTT, each block i has its own zeta_i
     // These correspond to powers of omega
     
     // Generate zetas for 64 blocks
     int32_t block_zetas[NUM_BLOCKS];
     for (int i = 0; i < NUM_BLOCKS; i++) {
         // zeta_i = omega^(i + 64) for negacyclic
         int32_t power = i + NUM_BLOCKS;
         int32_t zeta = 1;
         int32_t omega_mont = 13858533;  // omega in Montgomery form (from zetas_ntt[0])
         
         // Compute omega^power
         int32_t base = omega_mont;
         while (power > 0) {
             if (power & 1) {
                 zeta = fqmul(zeta, base);
             }
             base = fqmul(base, base);
             power >>= 1;
         }
         block_zetas[i] = zeta;
     }
     
     // Multiply each block
     for (int block = 0; block < NUM_BLOCKS; block++) {
         int offset = block * BLOCK_SIZE;
         int32_t zeta = block_zetas[block];
         
         // Schoolbook multiply 4×4
         int32_t tmp[4] = {0, 0, 0, 0};
         schoolbook_4x4_accumulate(tmp, &a[offset], &b[offset], zeta);
         
         // Copy result
         for (int i = 0; i < BLOCK_SIZE; i++) {
             r[offset + i] = tmp[i];
         }
     }
 }
 
 /**
  * pointwise_acc_ntt - Accumulate product in NTT domain (for matrix operations)
  * 
  * This is used in MatrixVectorMul to accumulate products:
  * r += a * b (in NTT domain)
  */
 static void pointwise_acc_ntt(int32_t r[SABER_N],
                               const int32_t a[SABER_N],
                               const int32_t b[SABER_N])
 {
     // Same as pointwise_mul_ntt but accumulates instead of replacing
     
     // This is THE KEY optimization from paper Section 3.1:
     // "Better Accumulation For Schoolbook Multiplication"
     
     int32_t block_zetas[NUM_BLOCKS];
     for (int i = 0; i < NUM_BLOCKS; i++) {
         int32_t power = i + NUM_BLOCKS;
         int32_t zeta = 1;
         int32_t base = 13858533;  // omega in Montgomery form
         
         while (power > 0) {
             if (power & 1) zeta = fqmul(zeta, base);
             base = fqmul(base, base);
             power >>= 1;
         }
         block_zetas[i] = zeta;
     }
     
     for (int block = 0; block < NUM_BLOCKS; block++) {
         int offset = block * BLOCK_SIZE;
         schoolbook_4x4_accumulate(&r[offset], &a[offset], &b[offset], block_zetas[block]);
     }
 }
 
 /* ========================================================================
  * Public API
  * ======================================================================== */
 
 /**
  * poly_mul - Multiply two polynomials
  * 
  * Computes: r = a * b in Z_8192[x]/(x^256 + 1)
  */
 void poly_mul(uint16_t r[SABER_N],
               const uint16_t a[SABER_N],
               const uint16_t b[SABER_N])
 {
     int32_t a_lift[SABER_N], b_lift[SABER_N], r_lift[SABER_N];
     
     // Lift Z_8192 → Z_q'
     poly_lift(a_lift, a);
     poly_lift(b_lift, b);
     
 #if USE_NEON
     // Forward NTT with NEON
     ntt_incomplete_forward_neon(a_lift);
     ntt_incomplete_forward_neon(b_lift);
 #else
     // Forward NTT scalar
     ntt_incomplete_forward(a_lift);
     ntt_incomplete_forward(b_lift);
 #endif
     
     // Pointwise multiplication in NTT domain
     pointwise_mul_ntt(r_lift, a_lift, b_lift);
     
 #if USE_NEON
     // Inverse NTT with NEON
     ntt_incomplete_inverse_neon(r_lift);
 #else
     // Inverse NTT scalar
     ntt_incomplete_inverse(r_lift);
 #endif
     
     // Reduce Z_q' → Z_8192
     poly_reduce_saber(r, r_lift);
 }
 
 /**
  * matrix_vector_mul - Matrix-vector multiplication with NTT
  * 
  * This is where the BIG speedup happens!
  * 
  * KEY OPTIMIZATION (from paper, Section 3.1):
  * Matrix A is l×l, vector s is l×1
  * 
  * NAIVE: Compute NTT(A[i][j]) and NTT(s[j]) for each multiplication
  * → 2*l^2 forward NTTs
  * 
  * OPTIMIZED: Precompute NTT(s[j]) once, reuse for all rows
  * → l forward NTTs for s, l^2 for A = l + l^2 forward NTTs
  * 
  * For l=3 (Saber): 18 → 12 NTTs (33% reduction!)
  * 
  * PLUS: Use accumulating schoolbook (Algorithm 18) to save adds
  * 
  * RESULT: 61% faster than Toom-Cook (from paper, Table 5)
  */
 void matrix_vector_mul(uint16_t r[SABER_L][SABER_N],
                       const uint16_t A[SABER_L][SABER_L][SABER_N],
                       const uint16_t s[SABER_L][SABER_N],
                       int transpose)
 {
     memset(r, 0, SABER_L * SABER_N * sizeof(uint16_t));
     
     // STEP 1: Precompute NTT(s[j]) - this is the key optimization!
     int32_t s_ntt[SABER_L][SABER_N];
     
     for (int j = 0; j < SABER_L; j++) {
         poly_lift(s_ntt[j], s[j]);
 #if USE_NEON
         ntt_incomplete_forward_neon(s_ntt[j]);
 #else
         ntt_incomplete_forward(s_ntt[j]);
 #endif
     }
     
     // STEP 2: Compute matrix-vector product in NTT domain
     int32_t r_ntt[SABER_L][SABER_N];
     memset(r_ntt, 0, sizeof(r_ntt));
     
     if (transpose) {
         // r = A^T × s
         for (int i = 0; i < SABER_L; i++) {
             for (int j = 0; j < SABER_L; j++) {
                 int32_t a_ntt[SABER_N];
                 
                 // Compute NTT(A[j][i])
                 poly_lift(a_ntt, A[j][i]);
 #if USE_NEON
                 ntt_incomplete_forward_neon(a_ntt);
 #else
                 ntt_incomplete_forward(a_ntt);
 #endif
                 
                 // Accumulate: r_ntt[i] += a_ntt * s_ntt[j]
                 pointwise_acc_ntt(r_ntt[i], a_ntt, s_ntt[j]);
             }
         }
     } else {
         // r = A × s
         for (int i = 0; i < SABER_L; i++) {
             for (int j = 0; j < SABER_L; j++) {
                 int32_t a_ntt[SABER_N];
                 
                 // Compute NTT(A[i][j])
                 poly_lift(a_ntt, A[i][j]);
 #if USE_NEON
                 ntt_incomplete_forward_neon(a_ntt);
 #else
                 ntt_incomplete_forward(a_ntt);
 #endif
                 
                 // Accumulate: r_ntt[i] += a_ntt * s_ntt[j]
                 pointwise_acc_ntt(r_ntt[i], a_ntt, s_ntt[j]);
             }
         }
     }
     
     // STEP 3: Inverse NTT and reduce
     for (int i = 0; i < SABER_L; i++) {
 #if USE_NEON
         ntt_incomplete_inverse_neon(r_ntt[i]);
 #else
         ntt_incomplete_inverse(r_ntt[i]);
 #endif
         poly_reduce_saber(r[i], r_ntt[i]);
     }
 }
 
 /**
  * inner_product - Inner product with NTT
  * 
  * Computes: r = a[0]*b[0] + a[1]*b[1] + ... + a[l-1]*b[l-1]
  * 
  * OPTIMIZATION: Same idea as matrix_vector_mul
  * Precompute NTT(b[i]), reuse for accumulation
  */
 void inner_product(uint16_t r[SABER_N],
                   const uint16_t a[SABER_L][SABER_N],
                   const uint16_t b[SABER_L][SABER_N])
 {
     memset(r, 0, SABER_N * sizeof(uint16_t));
     
     // Precompute NTT(b[i])
     int32_t b_ntt[SABER_L][SABER_N];
     for (int i = 0; i < SABER_L; i++) {
         poly_lift(b_ntt[i], b[i]);
 #if USE_NEON
         ntt_incomplete_forward_neon(b_ntt[i]);
 #else
         ntt_incomplete_forward(b_ntt[i]);
 #endif
     }
     
     // Accumulate products in NTT domain
     int32_t r_ntt[SABER_N];
     memset(r_ntt, 0, sizeof(r_ntt));
     
     for (int i = 0; i < SABER_L; i++) {
         int32_t a_ntt[SABER_N];
         
         poly_lift(a_ntt, a[i]);
 #if USE_NEON
         ntt_incomplete_forward_neon(a_ntt);
 #else
         ntt_incomplete_forward(a_ntt);
 #endif
         
         pointwise_acc_ntt(r_ntt, a_ntt, b_ntt[i]);
     }
     
     // Inverse NTT and reduce
 #if USE_NEON
     ntt_incomplete_inverse_neon(r_ntt);
 #else
     ntt_incomplete_inverse(r_ntt);
 #endif
     poly_reduce_saber(r, r_ntt);
 }
 
 /* ========================================================================
  * NOTES ON PERFORMANCE EXPECTATIONS
  * ======================================================================== */
 
 /*
  * From paper (TCHES 2021, Table 5):
  * 
  * Cortex-M4:
  * - MatrixVectorMul (l=3): 125k cycles (vs 317k Toom-Cook) → 61% faster
  * - InnerProduct (l=3): 57k cycles (vs 99k Toom-Cook) → 42% faster
  * - Overall Saber encapsulation: 22% faster
  * 
  * Apple M4 Max (expected, based on improved NEON):
  * - MatrixVectorMul: ~1.7-2.0× speedup vs Toom-Cook
  * - Overall KEM: ~1.20-1.30× speedup
  * 
  * KEY INSIGHTS:
  * 1. Incomplete NTT (6 layers) is optimal for Saber
  * 2. Precomputing NTT of reused operands is critical
  * 3. Optimized schoolbook with accumulation saves many ops
  * 4. NEON gives additional boost on ARM
  */